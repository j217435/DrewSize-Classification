# -*- coding: utf-8 -*-
"""ResNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sbV9k4F8Xhnh7nv5LNRcpxDbgO0QD2cg
"""

from google.colab import drive
drive.mount('/content/drive')

"""
Author: Joe
Date: 11-01-2022
Edited: 11-01-2022

Basic CNN for classification task

"""

import torch
from torch.nn import CrossEntropyLoss
from torch.utils.data.dataset import Dataset
from torchvision import datasets
import matplotlib.pyplot as plt
import os
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import torch.optim as optim
from tqdm import tqdm
import numpy as np
import torchvision 
import torch.utils.data as data
import os
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import argparse
import glob
import pandas as pd
from sklearn.model_selection import train_test_split
from PIL import Image
from torchsummary import summary

class_dict ={
    '20x26':0.0,
    '20x28':1.0,
    '28x22':2.0,
    '35x19':3.0,
    '35x22':4.0,
    '35x28':5.0,
    '35x30':6.0,
    '42x22':7.0,
    '42x30':8.0
}

image_list =[]
class_num = []
for name in glob.glob(r'/content/drive/MyDrive/Neocis_Ass/Image_gray/*'):
  for root, dirs, files in os.walk(name):
    for file in files:
      img_path = os.path.join(root,file)
      image_list.append(img_path)
      class_num.append(class_dict[f'{os.path.split(root)[-1]}'])

df = pd.DataFrame({
    'img':image_list,
    'class':class_num
})

# Generate the dataset
img_transform = transforms.Compose([
        transforms.Resize((224,224)),
        transforms.PILToTensor()
])
class DrillSize(Dataset):
    def __init__(self, image_root, class_num,transforms = img_transform): #transform = None):

        self.image_root = image_root
        self.class_num = class_num
        self.transform = transforms
        
    def __getitem__(self, index):

        image_ID = self.image_root[index]
        img_class = self.class_num[index]
        image = Image.open(image_ID)
        image = self.transform(image)
        # image=image.unsqueeze(0)
        #print('image:',image.shape)
        image_label = img_class                     
        #print('image_label:',image_label.shape)  
        return image, image_label
            
    def __len__(self):

        return len(self.image_root)

torch.manual_seed(0)
device  = torch.device('cuda:0' if torch.cuda.is_available() else'cpu')
print(device)

# training process
def train(model, model_name, optimizer, loss_metric, lr, epochs, train_dataloader, val_dataloader, **kwargs):
    net = model.to(device)
    optimizer = optimizer(net.parameters(), lr = lr)
    #loss_metric = loss_fcn
    val_loss_old = 10000
    val_loss_unchanged_counter = 0
    train_loss_list = []
    train_acc_list = []
    val_loss_list = []
    val_acc_list = []
    print("Start Training")
    for epoch in tqdm(range(epochs), total = epochs):
        print(f"==========Epoch:{epoch+1}==========lr:{lr}========{epoch+1}/{epochs}")
        loss_sum = 0
        train_batch_count = 0
        val_batch_count = 0
        train_correct = 0.0
        train_accuracy_total = 0
        for index, (img, label) in enumerate(train_dataloader):
            img, label = img.float(), label.float()
            img, label = img.to(device), label.to(device)
            optimizer.zero_grad()
            output = net.forward(img)
            loss = loss_metric(output, label.to(torch.int64))
            loss.backward()
            loss_sum += loss
            optimizer.step()
            _,predict = torch.max(output.data,dim=1)
            predict= predict.float()
            train_accuracy_total += label.size(0)
            print('train_predict:',predict)
            print('train_label:',label)
            train_correct += (predict.float() == label.float()).sum().item()
            print('train_correct:',train_correct)
            train_batch_count += 1

        train_acc = train_correct/train_accuracy_total
        train_loss = loss_sum.item()/train_accuracy_total
        train_loss_list.append(train_loss)
        train_acc_list.append(train_acc)
        model.eval()    
        with torch.no_grad():
            val_correct= 0
            loss_sum = 0
            val_accuracy_total = 0
            for index,( img, label) in enumerate(val_dataloader):
                img, label = img.float(), label.float()
                img, label = img.to(device), label.to(device)
                output = net.forward(img)
                loss = loss_metric(output, label.to(torch.int64))
                loss_sum += loss
                _,predict = torch.max(output.data,dim=1)
                predict= predict.float()
                print('val_predict:',predict)
                print('val_label:',label)
                val_accuracy_total += label.size(0)
                val_correct += (predict.float() == label.float()).sum().item()
                print('val_correct:',val_correct)
                val_batch_count = val_batch_count+1
            val_acc = val_correct/val_accuracy_total
            val_loss = loss_sum.item()/val_accuracy_total
            val_loss_list.append(val_loss)
            val_acc_list.append(val_acc)
            print('Epoch:',epoch+1)
            print('-'*20)
            print('Train Loss:',train_loss)
            print('Train Accuracy:',train_acc)
            print('-'*20)
            print('Validation Loss:',val_loss)
            print('Validation Accuracy:',val_acc)
            print('-'*20)
            torch.save({
            'epoch': epoch+1,
            'model': net,
            'model_state_dict': net.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'train_acc': train_acc_list,
            'val_acc': val_acc_list,
            'val_loss': val_loss_list,
            'train_loss': train_loss_list
            },os.path.join(MODEL_SAVE_DIR,"best_model_rest_net50.pt"))
            print("model saved")

# Data loader
image_root = df['img']
class_num = df['class']
train_set = DrillSize(image_root = image_root, class_num = class_num)
train_set_size = int(train_set.__len__()*0.8)
valid_set_size = len(train_set) - train_set_size
train_set, valid_set = data.random_split(train_set, [train_set_size, valid_set_size])
print("Train data set:",len(train_set))
print("Valid data set:", len(valid_set))

  
Batch_Size = 8
train_loader = DataLoader(dataset = train_set, batch_size = Batch_Size, shuffle = True)
valid_loader = DataLoader(dataset = valid_set, batch_size = Batch_Size, shuffle = True)

#load pretrained ResNet50
import torchvision.models as models

class ResNet50(nn.Module):
  def __init__(self, in_channels=1):
    super(ResNet50, self).__init__()
    self.model = models.resnet50(pretrained=True)
    self.model.conv1 = nn.Conv2d(in_channels, 64,3)
    
    # Change the output layer to output 9 classes instead of 1000 classes
    num_ftrs = self.model.fc.in_features
    self.model.fc = nn.Linear(num_ftrs, 9)

  def forward(self, x):
    x = self.model(x)
    return F.log_softmax(x,dim=1)

from torch.nn.modules.loss import CrossEntropyLoss
MODEL_SAVE_DIR = r'/content/drive/MyDrive/Neocis_Ass/Checkpoint'
LEARNING_RATE = 0.05
EPOCHS = 150
model = ResNet50()
torch.cuda.empty_cache()
model_name = "CNN_TEST"
loss_fcn = nn.NLLLoss()
optimizer = optim.Adam

#Start Training
out = train(model,
            model_name, 
            optimizer, 
            loss_fcn,
            LEARNING_RATE,
            epochs = EPOCHS,
            train_dataloader = train_loader,
            val_dataloader = valid_loader)

def load_checkpoint(filepath):
  checkpoint =torch.load(filepath)
  model = checkpoint['model']
  model.load_state_dict(checkpoint['model_state_dict'])
  optimizer = optim.Adam(model.parameters())
  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
  
  for parameter in model.parameters():
    parameter.requires_grad = False

  model.eval()
  return model, checkpoint

path =r'/content/drive/MyDrive/Neocis_Ass/Checkpoint/best_model_rest_net50.pt'

checkpoint =torch.load(path)
plt.figure()
plt.plot(checkpoint['train_acc'])
plt.plot(checkpoint['val_acc'])
#plt.plot(train_loss)
#plt.plot(val_loss)
plt.legend(["train_acc",'val_acc'])
plt.xlabel('epoch')
plt.ylabel('Accuracy')
plt.show